{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":10,"outputs":[{"output_type":"stream","text":"/kaggle/input/nlp-getting-started/train.csv\n/kaggle/input/nlp-getting-started/test.csv\n/kaggle/input/nlp-getting-started/sample_submission.csv\n","name":"stdout"}]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport string  \n\n\nimport nltk\nimport re\n\nimport scipy\nimport matplotlib.pyplot as plt\nfrom nltk.corpus import stopwords\nfrom nltk.tokenize import word_tokenize\nfrom nltk.stem import WordNetLemmatizer\n\n\nimport sklearn\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn import neighbors\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n\n\nfrom sklearn import metrics","execution_count":30,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train = pd.read_csv(\"/kaggle/input/nlp-getting-started/train.csv\")\ntest = pd.read_csv(\"/kaggle/input/nlp-getting-started/test.csv\")","execution_count":14,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df = train\ndf['text'] = df['text'].str.lower()   # pasamos a lowercase\ndf = df.drop(['id', 'location'], axis=1)","execution_count":15,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# generamos una columna que indica la cantidad de links a  enlaces externos\ndf['link'] = df['text'].apply(lambda x: x.count('http'))\n\n# generamos una columna que indica la cantidad de referencias a otras cuentas de twitter\ndf['contact'] = df['text'].apply(lambda x: x.count('@'))\n\n# generamos una columna que indica la cantidad de hashtags\ndf['hashtag'] = df['text'].apply(lambda x: x.count('#'))\n\n# generamos una columna que indica la cantidad de digitos\ndf['numerics'] = df['text'].apply(lambda x: len([x for x in x.split() if x.isdigit()]))\n\n# calculamos la longitud del tweet andes de limpiar\ndf['length'] = df['text'].str.len()","execution_count":16,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# calculamos la cantidad de palabras antes de limpiar\ndef count_words(text):\n    '''\n    Funcion que toma un texto y devuelve la cantidad de palabras\n    '''\n    word_counts = len(text.split(' '))\n    return word_counts\n\ndf['words'] = df['text'].apply(count_words)","execution_count":18,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# limpiamos el texto eliminando urls, cuentas, hashtags y numeros\n\ndef clean_str(string):\n    string = re.sub(r'https?\\://\\S+', '', string)\n    string = re.sub(r'http?\\://\\S+', '', string)\n    string = re.sub(r'@\\w*\\s', '', string)\n    string = re.sub(r'#\\w*\\s', '', string)\n    string = re.sub(r'\\d', '', string)\n    return string\n\ndf['text_clean'] = df['text'].apply(lambda x: clean_str(str(x)))","execution_count":19,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# eliminamos stopwords\n\nstop = stopwords.words('english')\n\ndf['text_clean'] = df['text_clean'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stop)]))","execution_count":23,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# eliminamos signos de puntuacion y caracteres especiales\ndf['text_clean'] = df['text_clean'].str.replace('[^\\w\\s]','')","execution_count":24,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df['word_tokenize'] = df['text_clean'].apply(lambda x: word_tokenize(x))","execution_count":27,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def word_lemmatizer(text):\n    lem_text = [WordNetLemmatizer().lemmatize(i) for i in text]\n    return lem_text","execution_count":28,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# unificamos palabras que poseen la misma raiz aplicando la funcion word_lematizer\ndf['word_lemmatizer'] = df['word_tokenize'].apply(lambda x: word_lemmatizer(x))","execution_count":31,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# unificamos la lista de tokens para poder analizar el texto limpio\ndf['text_clean'] = df['word_lemmatizer'].str.join(' ')","execution_count":32,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# calculamos nuevamente la longuitud, pero ahora del texto limpio                       \ndf['length-clean'] = df['text_clean'].str.len()","execution_count":33,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df['words_clean'] = df['text_clean'].apply(count_words)","execution_count":34,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df['keyword'] = df.keyword.str.replace('%20', '_')\ndf['keyword'] = df.keyword.str.lower()","execution_count":35,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# aplicamos TF-IDF seteando un maximo de 1500 palabras\ntfidf = TfidfVectorizer(max_features=1500, lowercase=True, analyzer='word', stop_words= 'english',ngram_range=(1,1))\n\n\ntrain_vect = tfidf.fit_transform(df['text_clean'])                                             \n\n# lo pasamos a dataframe\ndf_tf_idf = pd.DataFrame(data = train_vect.todense(), columns = tfidf.get_feature_names())","execution_count":36,"outputs":[{"output_type":"error","ename":"NameError","evalue":"name 'TfidfVectorizer' is not defined","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-36-e0c942597b3a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# aplicamos TF-IDF seteando un maximo de 1500 palabras\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtfidf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTfidfVectorizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_features\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1500\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlowercase\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0manalyzer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'word'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstop_words\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0;34m'english'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mngram_range\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mtrain_vect\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtfidf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'text_clean'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'TfidfVectorizer' is not defined"]}]},{"metadata":{"trusted":true},"cell_type":"code","source":"# como existe la columna target en el analisis tf-idf, la modifico para hacer el concat con 'target' y que no se duplique\ndf_tf_idf['targ'] = df_tf_idf['target']\ndf_tf_idf = df_tf_idf.drop(['target'], axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# eliminamos col que contienen texto y conservamos unicamente las numericas\ndf_num = df.drop([ 'keyword', 'text', 'text_clean', 'word_tokenize', 'word_lemmatizer'], axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train = pd.concat([df_num, df_tf_idf], axis=1)  # revisar porque aparecen dos columnas llamadas target","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# separamos el target del resto de los features\n\ny = df_train.target    \nX = df_train.drop('target', axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# hacemos division entre train y test para cross validation\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=7)      \n\nX_train.shape, X_test.shape, y_train.shape, y_test.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# estandarizamos las features\nscaler = StandardScaler()\n\nX_train = scaler.fit_transform(X_train)\n\nX_test = scaler.transform(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"classifier = neighbors.KNeighborsClassifier()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"classifier.fit(X_train,y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"classifier.score(X_train, y_train), classifier.score(X_test,y_test)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":4}