{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":1,"outputs":[{"output_type":"stream","text":"/kaggle/input/nlp-getting-started/train.csv\n/kaggle/input/nlp-getting-started/test.csv\n/kaggle/input/nlp-getting-started/sample_submission.csv\n","name":"stdout"}]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport string  \n\n\nimport nltk\nimport re\n\nimport scipy\nimport matplotlib.pyplot as plt\nfrom nltk.corpus import stopwords\nfrom nltk.tokenize import word_tokenize\nfrom nltk.stem import WordNetLemmatizer\n\n\nimport sklearn\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn import neighbors\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n\n\nfrom sklearn import metrics","execution_count":2,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train = pd.read_csv(\"/kaggle/input/nlp-getting-started/train.csv\")\ntest = pd.read_csv(\"/kaggle/input/nlp-getting-started/test.csv\")","execution_count":3,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df = train\ndf['text'] = df['text'].str.lower()   # pasamos a lowercase\ndf = df.drop(['id', 'location'], axis=1)","execution_count":4,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# generamos una columna que indica la cantidad de links a  enlaces externos\ndf['link'] = df['text'].apply(lambda x: x.count('http'))\n\n# generamos una columna que indica la cantidad de referencias a otras cuentas de twitter\ndf['contact'] = df['text'].apply(lambda x: x.count('@'))\n\n# generamos una columna que indica la cantidad de hashtags\ndf['hashtag'] = df['text'].apply(lambda x: x.count('#'))\n\n# generamos una columna que indica la cantidad de digitos\ndf['numerics'] = df['text'].apply(lambda x: len([x for x in x.split() if x.isdigit()]))\n\n# calculamos la longitud del tweet andes de limpiar\ndf['length'] = df['text'].str.len()","execution_count":5,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# calculamos la cantidad de palabras antes de limpiar\ndef count_words(text):\n    '''\n    Funcion que toma un texto y devuelve la cantidad de palabras\n    '''\n    word_counts = len(text.split(' '))\n    return word_counts\n\ndf['words'] = df['text'].apply(count_words)","execution_count":6,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# limpiamos el texto eliminando urls, cuentas, hashtags y numeros\n\ndef clean_str(string):\n    string = re.sub(r'https?\\://\\S+', '', string)\n    string = re.sub(r'http?\\://\\S+', '', string)\n    string = re.sub(r'@\\w*\\s', '', string)\n    string = re.sub(r'#\\w*\\s', '', string)\n    string = re.sub(r'\\d', '', string)\n    return string\n\ndf['text_clean'] = df['text'].apply(lambda x: clean_str(str(x)))","execution_count":7,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# eliminamos stopwords\n\nstop = stopwords.words('english')\n\ndf['text_clean'] = df['text_clean'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stop)]))","execution_count":8,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# eliminamos signos de puntuacion y caracteres especiales\ndf['text_clean'] = df['text_clean'].str.replace('[^\\w\\s]','')","execution_count":10,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df['word_tokenize'] = df['text_clean'].apply(lambda x: word_tokenize(x))","execution_count":11,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def word_lemmatizer(text):\n    lem_text = [WordNetLemmatizer().lemmatize(i) for i in text]\n    return lem_text","execution_count":12,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# unificamos palabras que poseen la misma raiz aplicando la funcion word_lematizer\ndf['word_lemmatizer'] = df['word_tokenize'].apply(lambda x: word_lemmatizer(x))","execution_count":13,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# unificamos la lista de tokens para poder analizar el texto limpio\ndf['text_clean'] = df['word_lemmatizer'].str.join(' ')","execution_count":14,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# calculamos nuevamente la longuitud, pero ahora del texto limpio                       \ndf['length-clean'] = df['text_clean'].str.len()","execution_count":15,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df['words_clean'] = df['text_clean'].apply(count_words)","execution_count":16,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df['keyword'] = df.keyword.str.replace('%20', '_')\ndf['keyword'] = df.keyword.str.lower()","execution_count":17,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# aplicamos TF-IDF seteando un maximo de 1500 palabras\ntfidf = TfidfVectorizer(max_features=1500, lowercase=True, analyzer='word', stop_words= 'english',ngram_range=(1,1))\n\n\ntrain_vect = tfidf.fit_transform(df['text_clean'])                                             \n\n# lo pasamos a dataframe\ndf_tf_idf = pd.DataFrame(data = train_vect.todense(), columns = tfidf.get_feature_names())","execution_count":18,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# como existe la columna target en el analisis tf-idf, la modifico para hacer el concat con 'target' y que no se duplique\ndf_tf_idf['targ'] = df_tf_idf['target']\ndf_tf_idf = df_tf_idf.drop(['target'], axis=1)","execution_count":19,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# eliminamos col que contienen texto y conservamos unicamente las numericas\ndf_num = df.drop([ 'keyword', 'text', 'text_clean', 'word_tokenize', 'word_lemmatizer'], axis=1)","execution_count":20,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train = pd.concat([df_num, df_tf_idf], axis=1)  # revisar porque aparecen dos columnas llamadas target","execution_count":21,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# separamos el target del resto de los features\n\ny = df_train.target    \nX = df_train.drop('target', axis=1)","execution_count":22,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# hacemos division entre train y test para cross validation\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=7)      \n\nX_train.shape, X_test.shape, y_train.shape, y_test.shape","execution_count":23,"outputs":[{"output_type":"execute_result","execution_count":23,"data":{"text/plain":"((5329, 1508), (2284, 1508), (5329,), (2284,))"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"classifier = neighbors.KNeighborsClassifier()","execution_count":25,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"classifier.fit(X_train,y_train)","execution_count":26,"outputs":[{"output_type":"execute_result","execution_count":26,"data":{"text/plain":"KNeighborsClassifier()"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"classifier.score(X_train, y_train), classifier.score(X_test,y_test)","execution_count":27,"outputs":[{"output_type":"execute_result","execution_count":27,"data":{"text/plain":"(0.7470444736348283, 0.6401050788091068)"},"metadata":{}}]}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":4}