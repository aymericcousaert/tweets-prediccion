{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":14,"outputs":[{"output_type":"stream","text":"/kaggle/input/nlp-getting-started/test.csv\n/kaggle/input/nlp-getting-started/sample_submission.csv\n/kaggle/input/nlp-getting-started/train.csv\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport re\nfrom nltk.tokenize import word_tokenize\nfrom nltk import pos_tag\nfrom nltk.corpus import stopwords\nfrom nltk.stem import WordNetLemmatizer\nfrom sklearn.preprocessing import LabelEncoder\nfrom collections import defaultdict\nfrom nltk.corpus import wordnet as wn\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn import model_selection, naive_bayes, svm\nfrom sklearn.metrics import accuracy_score","execution_count":15,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"train = pd.read_csv('/kaggle/input/nlp-getting-started/train.csv')\ntest = pd.read_csv('/kaggle/input/nlp-getting-started/test.csv')","execution_count":16,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df = train","execution_count":47,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df['location'] = df.location.apply(lambda x: '' if x != x else x.lower())\ndf.loc[df.location == 'new York, ny','location'] = 'new york'\ndf.loc[df.location == 'nyc','location'] = 'new york'\ndf.loc[df.location == 'ny','location'] = 'new york'\ndf.loc[df.location == 'new york city','location'] = 'new york'\ndf.loc[df.location == 'us','location'] = 'united states'\ndf.loc[df.location == 'usa','location'] = 'united states'\ndf.loc[df.location == 'california','location'] = 'united states'\ndf.loc[df.location == 'southern california','location'] = 'united states'\ndf.loc[df.location == 'orange county','location'] = 'united states'\ndf.loc[df.location == 'u.s.a','location'] = 'united states'\ndf.loc[df.location == 'ca','location'] = 'united states'\ndf.loc[df.location == 'san francisco bay area','location'] = 'san francisco'\ndf.loc[df.location == 'uk','location'] = 'united kingdom'","execution_count":48,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.loc[df.location == 'ss','location'] = ''\ndf.loc[df.location == 'everywhere','location'] = ''\ndf.loc[df.location == 'earth','location'] = ''\ndf.loc[df.location == '304','location'] = ''\ndf.loc[df.location == 'world','location'] = ''\ndf.loc[df.location == 'worldwide','location'] = ''\ndf.loc[df.location == '??????','location'] = ''\ndf.loc[df.location == 'pedophile hunting ground','location'] = ''\ndf.loc[df.location == 'planet earth','location'] = ''\ndf.loc[df.location == 'global','location'] = ''\ndf.loc[df.location == 'in the word of god','location'] = ''\ndf.loc[df.location == '?','location'] = ''\ndf.loc[df.location == 'mad as hell','location'] = ''\ndf.loc[df.location == '??','location'] = ''\ndf.loc[df.location == '?????','location'] = ''\ndf.loc[df.location == 'happily married with 2 kids','location'] = ''\ndf.loc[df.location == 'south','location'] = ''\ndf.loc[df.location == 'world wide','location'] = ''","execution_count":49,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df = df.replace(np.nan, '', regex=True)\ndf['keyword'] = df.keyword.str.replace('%20', '_')\ndf['keyword'] = df.keyword.str.lower()\ndf['text'] = df['keyword'] + ' ' + df['location'] + ' ' + df['text']","execution_count":50,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.sample(10)","execution_count":51,"outputs":[{"output_type":"execute_result","execution_count":51,"data":{"text/plain":"        id             keyword                     location  \\\n1542  2229  chemical_emergency          seattle, washington   \n4853  6912       mass_murderer                                \n2713  3897          detonation                                \n1842  2648             crashed               international    \n4729  6725                lava                united states   \n2982  4282            drowning                  yorkshire\\n   \n5184  7400         obliterated                     new york   \n6405  9156      suicide_bomber              garden city, ny   \n3836  5459    first_responders  franklin, tn near nashville   \n5130  7317     nuclear_reactor                   denver, co   \n\n                                                   text  target  \n1542  chemical_emergency seattle, washington Downtow...       0  \n4853  mass_murderer  @atljw @cnnbrk fine line btw ma...       0  \n2713  detonation  Ignition Knock (Detonation) Sensor...       0  \n1842  crashed international  TTW Today's News: Bin L...       1  \n4729  lava united states Deal of The Day : http://t....       0  \n2982  drowning yorkshire\\n #Islamic #state issue a n...       0  \n5184                   obliterated new york Obliterated       0  \n6405  suicide_bomber garden city, ny Suicide bomber ...       1  \n3836  first_responders franklin, tn near nashville A...       1  \n5130  nuclear_reactor denver, co Nuclear #Solar Powe...       1  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>keyword</th>\n      <th>location</th>\n      <th>text</th>\n      <th>target</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>1542</th>\n      <td>2229</td>\n      <td>chemical_emergency</td>\n      <td>seattle, washington</td>\n      <td>chemical_emergency seattle, washington Downtow...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4853</th>\n      <td>6912</td>\n      <td>mass_murderer</td>\n      <td></td>\n      <td>mass_murderer  @atljw @cnnbrk fine line btw ma...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2713</th>\n      <td>3897</td>\n      <td>detonation</td>\n      <td></td>\n      <td>detonation  Ignition Knock (Detonation) Sensor...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1842</th>\n      <td>2648</td>\n      <td>crashed</td>\n      <td>international</td>\n      <td>crashed international  TTW Today's News: Bin L...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>4729</th>\n      <td>6725</td>\n      <td>lava</td>\n      <td>united states</td>\n      <td>lava united states Deal of The Day : http://t....</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2982</th>\n      <td>4282</td>\n      <td>drowning</td>\n      <td>yorkshire\\n</td>\n      <td>drowning yorkshire\\n #Islamic #state issue a n...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>5184</th>\n      <td>7400</td>\n      <td>obliterated</td>\n      <td>new york</td>\n      <td>obliterated new york Obliterated</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>6405</th>\n      <td>9156</td>\n      <td>suicide_bomber</td>\n      <td>garden city, ny</td>\n      <td>suicide_bomber garden city, ny Suicide bomber ...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>3836</th>\n      <td>5459</td>\n      <td>first_responders</td>\n      <td>franklin, tn near nashville</td>\n      <td>first_responders franklin, tn near nashville A...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>5130</th>\n      <td>7317</td>\n      <td>nuclear_reactor</td>\n      <td>denver, co</td>\n      <td>nuclear_reactor denver, co Nuclear #Solar Powe...</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"df = df.drop(['id','keyword','location'], axis=1)","execution_count":52,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def clean_str(string):\n    string = re.sub(r'https?\\://\\S+', '', string)\n    string = re.sub(r'http?\\://\\S+', '', string)\n    string = re.sub(r'@\\w*\\s', '', string)\n    string = re.sub(r'#\\w*\\s', '', string)\n    string = re.sub(r'\\d', '', string)\n    return string\n\ndf['text'] = df['text'].apply(lambda x: clean_str(str(x)))","execution_count":53,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df['text'].dropna(inplace=True)\ndf['text'] = [entry.lower() for entry in df['text']]\ndf['text']= [word_tokenize(entry) for entry in df['text']]\ntag_map = defaultdict(lambda : wn.NOUN)\ntag_map['J'] = wn.ADJ\ntag_map['V'] = wn.VERB\ntag_map['R'] = wn.ADV\nfor index,entry in enumerate(df['text']):\n    Final_words = []\n    word_Lemmatized = WordNetLemmatizer()\n    for word, tag in pos_tag(entry):\n        if word not in stopwords.words('english') and word.isalpha():\n            word_Final = word_Lemmatized.lemmatize(word,tag_map[tag[0]])\n            Final_words.append(word_Final)\n    df.loc[index,'text_final'] = str(Final_words)","execution_count":54,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train, X_test, y_train, y_test = model_selection.train_test_split(df['text_final'],df['target'],test_size=0.33)","execution_count":109,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Tfidf_vect = TfidfVectorizer(max_features=4000)\nTfidf_vect.fit(df['text_final'])\nX_train_Tfidf = Tfidf_vect.transform(X_train)\nX_test_Tfidf = Tfidf_vect.transform(X_test)","execution_count":110,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Naive = naive_bayes.MultinomialNB()\nNaive.fit(X_train_Tfidf,y_train)","execution_count":111,"outputs":[{"output_type":"execute_result","execution_count":111,"data":{"text/plain":"MultinomialNB()"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Naive Bayes Accuracy Score on test : \",accuracy_score(Naive.predict(X_test_Tfidf), y_test))","execution_count":112,"outputs":[{"output_type":"stream","text":"Naive Bayes Accuracy Score on test :  0.8026263430163152\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"SVM = svm.SVC(C=1.0, kernel='sigmoid', degree=3, gamma='scale')\nSVM.fit(X_train_Tfidf,y_train)","execution_count":113,"outputs":[{"output_type":"execute_result","execution_count":113,"data":{"text/plain":"SVC(kernel='sigmoid')"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"SVM Accuracy Score on test : \",accuracy_score(SVM.predict(X_test_Tfidf), y_test))","execution_count":114,"outputs":[{"output_type":"stream","text":"SVM Accuracy Score on test :  0.7922801432550736\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"**Submit kaggle**"},{"metadata":{"trusted":true},"cell_type":"code","source":"df = test","execution_count":115,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df['location'] = df.location.apply(lambda x: '' if x != x else x.lower())\ndf.loc[df.location == 'new York, ny','location'] = 'new york'\ndf.loc[df.location == 'nyc','location'] = 'new york'\ndf.loc[df.location == 'ny','location'] = 'new york'\ndf.loc[df.location == 'new york city','location'] = 'new york'\ndf.loc[df.location == 'us','location'] = 'united states'\ndf.loc[df.location == 'usa','location'] = 'united states'\ndf.loc[df.location == 'california','location'] = 'united states'\ndf.loc[df.location == 'southern california','location'] = 'united states'\ndf.loc[df.location == 'orange county','location'] = 'united states'\ndf.loc[df.location == 'u.s.a','location'] = 'united states'\ndf.loc[df.location == 'ca','location'] = 'united states'\ndf.loc[df.location == 'san francisco bay area','location'] = 'san francisco'\ndf.loc[df.location == 'uk','location'] = 'united kingdom'","execution_count":116,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.loc[df.location == 'ss','location'] = ''\ndf.loc[df.location == 'everywhere','location'] = ''\ndf.loc[df.location == 'earth','location'] = ''\ndf.loc[df.location == '304','location'] = ''\ndf.loc[df.location == 'world','location'] = ''\ndf.loc[df.location == 'worldwide','location'] = ''\ndf.loc[df.location == '??????','location'] = np.nan\ndf.loc[df.location == 'pedophile hunting ground','location'] = ''\ndf.loc[df.location == 'planet earth','location'] = ''\ndf.loc[df.location == 'global','location'] = ''\ndf.loc[df.location == 'in the word of god','location'] = ''\ndf.loc[df.location == '?','location'] = ''\ndf.loc[df.location == 'mad as hell','location'] = ''\ndf.loc[df.location == '??','location'] = ''\ndf.loc[df.location == '?????','location'] = ''\ndf.loc[df.location == 'happily married with 2 kids','location'] = ''\ndf.loc[df.location == 'south','location'] = ''\ndf.loc[df.location == 'world wide','location'] = ''","execution_count":117,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df = df.replace(np.nan, '', regex=True)\ndf['keyword'] = df.keyword.str.replace('%20', '_')\ndf['keyword'] = df.keyword.str.lower()\ndf['text'] = df['keyword'] + ' ' + df['location'] + ' ' + df['text']","execution_count":118,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df = df.drop(['id','keyword','location'], axis=1)","execution_count":119,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def clean_str(string):\n    string = re.sub(r'https?\\://\\S+', '', string)\n    string = re.sub(r'http?\\://\\S+', '', string)\n    string = re.sub(r'@\\w*\\s', '', string)\n    string = re.sub(r'#\\w*\\s', '', string)\n    string = re.sub(r'\\d', '', string)\n    return string\n\ndf['text'] = df['text'].apply(lambda x: clean_str(str(x)))","execution_count":120,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df['text'].dropna(inplace=True)\ndf['text'] = [entry.lower() for entry in df['text']]\ndf['text']= [word_tokenize(entry) for entry in df['text']]\ntag_map = defaultdict(lambda : wn.NOUN)\ntag_map['J'] = wn.ADJ\ntag_map['V'] = wn.VERB\ntag_map['R'] = wn.ADV\nfor index,entry in enumerate(df['text']):\n    Final_words = []\n    word_Lemmatized = WordNetLemmatizer()\n    for word, tag in pos_tag(entry):\n        if word not in stopwords.words('english') and word.isalpha():\n            word_Final = word_Lemmatized.lemmatize(word,tag_map[tag[0]])\n            Final_words.append(word_Final)\n    df.loc[index,'text_final'] = str(Final_words)","execution_count":121,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_test_Tfidf = Tfidf_vect.transform(df['text_final'])","execution_count":122,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"predictions_nb = Naive.predict(X_test_Tfidf)","execution_count":130,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"predictions_nb = pd.DataFrame(predictions_nb, columns=['target'])","execution_count":131,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test['target'] = predictions_nb['target']","execution_count":132,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"results = test[['id', 'target']]","execution_count":133,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"results.to_csv('/kaggle/working/results_1.csv', index=False)","execution_count":129,"outputs":[]}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":4}