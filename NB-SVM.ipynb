{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":21,"outputs":[{"output_type":"stream","text":"/kaggle/input/nlp-getting-started/sample_submission.csv\n/kaggle/input/nlp-getting-started/test.csv\n/kaggle/input/nlp-getting-started/train.csv\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport re\nfrom nltk.tokenize import word_tokenize\nfrom nltk import pos_tag\nfrom nltk.corpus import stopwords\nfrom nltk.stem import WordNetLemmatizer\nfrom sklearn.preprocessing import LabelEncoder\nfrom collections import defaultdict\nfrom nltk.corpus import wordnet as wn\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn import model_selection, naive_bayes, svm\nfrom sklearn.metrics import accuracy_score","execution_count":22,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"train = pd.read_csv('/kaggle/input/nlp-getting-started/train.csv')\ntest = pd.read_csv('/kaggle/input/nlp-getting-started/test.csv')","execution_count":23,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df = train","execution_count":24,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df['location'] = df.location.apply(lambda x: '' if x != x else x.lower())\ndf.loc[df.location == 'new York, ny','location'] = 'new york'\ndf.loc[df.location == 'nyc','location'] = 'new york'\ndf.loc[df.location == 'ny','location'] = 'new york'\ndf.loc[df.location == 'new york city','location'] = 'new york'\ndf.loc[df.location == 'us','location'] = 'united states'\ndf.loc[df.location == 'usa','location'] = 'united states'\ndf.loc[df.location == 'california','location'] = 'united states'\ndf.loc[df.location == 'southern california','location'] = 'united states'\ndf.loc[df.location == 'orange county','location'] = 'united states'\ndf.loc[df.location == 'u.s.a','location'] = 'united states'\ndf.loc[df.location == 'ca','location'] = 'united states'\ndf.loc[df.location == 'san francisco bay area','location'] = 'san francisco'\ndf.loc[df.location == 'uk','location'] = 'united kingdom'","execution_count":25,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.loc[df.location == 'ss','location'] = ''\ndf.loc[df.location == 'everywhere','location'] = ''\ndf.loc[df.location == 'earth','location'] = ''\ndf.loc[df.location == '304','location'] = ''\ndf.loc[df.location == 'world','location'] = ''\ndf.loc[df.location == 'worldwide','location'] = ''\ndf.loc[df.location == '??????','location'] = ''\ndf.loc[df.location == 'pedophile hunting ground','location'] = ''\ndf.loc[df.location == 'planet earth','location'] = ''\ndf.loc[df.location == 'global','location'] = ''\ndf.loc[df.location == 'in the word of god','location'] = ''\ndf.loc[df.location == '?','location'] = ''\ndf.loc[df.location == 'mad as hell','location'] = ''\ndf.loc[df.location == '??','location'] = ''\ndf.loc[df.location == '?????','location'] = ''\ndf.loc[df.location == 'happily married with 2 kids','location'] = ''\ndf.loc[df.location == 'south','location'] = ''\ndf.loc[df.location == 'world wide','location'] = ''","execution_count":26,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df = df.replace(np.nan, '', regex=True)\ndf['keyword'] = df.keyword.str.replace('%20', '_')\ndf['keyword'] = df.keyword.str.lower()\ndf['text'] = df['keyword'] + ' ' + df['location'] + ' ' + df['text']","execution_count":27,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.sample(10)","execution_count":28,"outputs":[{"output_type":"execute_result","execution_count":28,"data":{"text/plain":"        id      keyword                   location  \\\n1450  2091     casualty                              \n1997  2871       damage  http://twitch.tv/jcmonkey   \n4836  6883  mass_murder      by a piano probably.    \n878   1272        blood              ewa beach, hi   \n6229  8892    snowstorm                 dallas, tx   \n5833  8334       rubble      made here in detroit    \n5687  8116      rescued              united states   \n5203  7430  obliterated                     canada   \n6541  9360     survived                              \n3546  5069       famine                   panamìá    \n\n                                                   text  target  \n1450          casualty  @5SOSFamUpdater social casualty       0  \n1997  damage http://twitch.tv/jcmonkey @Drothvader @...       1  \n4836  mass_murder by a piano probably.  @yelllowheat...       1  \n878      blood ewa beach, hi @Anthxvy runs in the blood       0  \n6229  snowstorm dallas, tx 'Snowstorm' 36'x36' oil o...       0  \n5833  rubble made here in detroit  #360WiseNews : Ch...       1  \n5687  rescued united states A brave little dog gets ...       0  \n5203  obliterated canada @Silverhusky Shtap!  Before...       0  \n6541  survived  Not even the Berlin wall survived th...       0  \n3546  famine panamìá  Top 3:\\n* Turn on the Darkness...       0  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>keyword</th>\n      <th>location</th>\n      <th>text</th>\n      <th>target</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>1450</th>\n      <td>2091</td>\n      <td>casualty</td>\n      <td></td>\n      <td>casualty  @5SOSFamUpdater social casualty</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1997</th>\n      <td>2871</td>\n      <td>damage</td>\n      <td>http://twitch.tv/jcmonkey</td>\n      <td>damage http://twitch.tv/jcmonkey @Drothvader @...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>4836</th>\n      <td>6883</td>\n      <td>mass_murder</td>\n      <td>by a piano probably.</td>\n      <td>mass_murder by a piano probably.  @yelllowheat...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>878</th>\n      <td>1272</td>\n      <td>blood</td>\n      <td>ewa beach, hi</td>\n      <td>blood ewa beach, hi @Anthxvy runs in the blood</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>6229</th>\n      <td>8892</td>\n      <td>snowstorm</td>\n      <td>dallas, tx</td>\n      <td>snowstorm dallas, tx 'Snowstorm' 36'x36' oil o...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>5833</th>\n      <td>8334</td>\n      <td>rubble</td>\n      <td>made here in detroit</td>\n      <td>rubble made here in detroit  #360WiseNews : Ch...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>5687</th>\n      <td>8116</td>\n      <td>rescued</td>\n      <td>united states</td>\n      <td>rescued united states A brave little dog gets ...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>5203</th>\n      <td>7430</td>\n      <td>obliterated</td>\n      <td>canada</td>\n      <td>obliterated canada @Silverhusky Shtap!  Before...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>6541</th>\n      <td>9360</td>\n      <td>survived</td>\n      <td></td>\n      <td>survived  Not even the Berlin wall survived th...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3546</th>\n      <td>5069</td>\n      <td>famine</td>\n      <td>panamìá</td>\n      <td>famine panamìá  Top 3:\\n* Turn on the Darkness...</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"df = df.drop(['id','keyword','location'], axis=1)","execution_count":29,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def clean_str(string):\n    string = re.sub(r'https?\\://\\S+', '', string)\n    string = re.sub(r'http?\\://\\S+', '', string)\n    string = re.sub(r'@\\w*\\s', '', string)\n    string = re.sub(r'#\\w*\\s', '', string)\n    string = re.sub(r'\\d', '', string)\n    return string\n\ndf['text'] = df['text'].apply(lambda x: clean_str(str(x)))","execution_count":30,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df['text'].dropna(inplace=True)\ndf['text'] = [entry.lower() for entry in df['text']]\ndf['text']= [word_tokenize(entry) for entry in df['text']]\ntag_map = defaultdict(lambda : wn.NOUN)\ntag_map['J'] = wn.ADJ\ntag_map['V'] = wn.VERB\ntag_map['R'] = wn.ADV\nfor index,entry in enumerate(df['text']):\n    Final_words = []\n    word_Lemmatized = WordNetLemmatizer()\n    for word, tag in pos_tag(entry):\n        if word not in stopwords.words('english') and word.isalpha():\n            word_Final = word_Lemmatized.lemmatize(word,tag_map[tag[0]])\n            Final_words.append(word_Final)\n    df.loc[index,'text_final'] = str(Final_words)","execution_count":31,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train, X_test, y_train, y_test = model_selection.train_test_split(df['text_final'],df['target'],test_size=0.33)","execution_count":32,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Tfidf_vect = TfidfVectorizer(max_features=4000)\nTfidf_vect.fit(df['text_final'])\nX_train_Tfidf = Tfidf_vect.transform(X_train)\nX_test_Tfidf = Tfidf_vect.transform(X_test)","execution_count":33,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Naive = naive_bayes.MultinomialNB()\nNaive.fit(X_train_Tfidf,y_train)","execution_count":34,"outputs":[{"output_type":"execute_result","execution_count":34,"data":{"text/plain":"MultinomialNB()"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Naive Bayes Accuracy Score on train : \",accuracy_score(Naive.predict(X_train_Tfidf), y_train))\nprint(\"Naive Bayes Accuracy Score on test : \",accuracy_score(Naive.predict(X_test_Tfidf), y_test))","execution_count":35,"outputs":[{"output_type":"stream","text":"Naive Bayes Accuracy Score on train :  0.8523529411764705\nNaive Bayes Accuracy Score on test :  0.7946677278153601\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"SVM = svm.SVC(C=1.0, kernel='sigmoid', degree=3, gamma='scale')\nSVM.fit(X_train_Tfidf,y_train)","execution_count":36,"outputs":[{"output_type":"execute_result","execution_count":36,"data":{"text/plain":"SVC(kernel='sigmoid')"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"SVM Accuracy Score on test : \",accuracy_score(SVM.predict(X_train_Tfidf), y_train))\nprint(\"SVM Accuracy Score on test : \",accuracy_score(SVM.predict(X_test_Tfidf), y_test))","execution_count":37,"outputs":[{"output_type":"stream","text":"SVM Accuracy Score on test :  0.8345098039215686\nSVM Accuracy Score on test :  0.7883008356545961\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"**Submit kaggle**"},{"metadata":{"trusted":true},"cell_type":"code","source":"df = test","execution_count":38,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df['location'] = df.location.apply(lambda x: '' if x != x else x.lower())\ndf.loc[df.location == 'new York, ny','location'] = 'new york'\ndf.loc[df.location == 'nyc','location'] = 'new york'\ndf.loc[df.location == 'ny','location'] = 'new york'\ndf.loc[df.location == 'new york city','location'] = 'new york'\ndf.loc[df.location == 'us','location'] = 'united states'\ndf.loc[df.location == 'usa','location'] = 'united states'\ndf.loc[df.location == 'california','location'] = 'united states'\ndf.loc[df.location == 'southern california','location'] = 'united states'\ndf.loc[df.location == 'orange county','location'] = 'united states'\ndf.loc[df.location == 'u.s.a','location'] = 'united states'\ndf.loc[df.location == 'ca','location'] = 'united states'\ndf.loc[df.location == 'san francisco bay area','location'] = 'san francisco'\ndf.loc[df.location == 'uk','location'] = 'united kingdom'","execution_count":39,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.loc[df.location == 'ss','location'] = ''\ndf.loc[df.location == 'everywhere','location'] = ''\ndf.loc[df.location == 'earth','location'] = ''\ndf.loc[df.location == '304','location'] = ''\ndf.loc[df.location == 'world','location'] = ''\ndf.loc[df.location == 'worldwide','location'] = ''\ndf.loc[df.location == '??????','location'] = np.nan\ndf.loc[df.location == 'pedophile hunting ground','location'] = ''\ndf.loc[df.location == 'planet earth','location'] = ''\ndf.loc[df.location == 'global','location'] = ''\ndf.loc[df.location == 'in the word of god','location'] = ''\ndf.loc[df.location == '?','location'] = ''\ndf.loc[df.location == 'mad as hell','location'] = ''\ndf.loc[df.location == '??','location'] = ''\ndf.loc[df.location == '?????','location'] = ''\ndf.loc[df.location == 'happily married with 2 kids','location'] = ''\ndf.loc[df.location == 'south','location'] = ''\ndf.loc[df.location == 'world wide','location'] = ''","execution_count":40,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df = df.replace(np.nan, '', regex=True)\ndf['keyword'] = df.keyword.str.replace('%20', '_')\ndf['keyword'] = df.keyword.str.lower()\ndf['text'] = df['keyword'] + ' ' + df['location'] + ' ' + df['text']","execution_count":41,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df = df.drop(['id','keyword','location'], axis=1)","execution_count":42,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def clean_str(string):\n    string = re.sub(r'https?\\://\\S+', '', string)\n    string = re.sub(r'http?\\://\\S+', '', string)\n    string = re.sub(r'@\\w*\\s', '', string)\n    string = re.sub(r'#\\w*\\s', '', string)\n    string = re.sub(r'\\d', '', string)\n    return string\n\ndf['text'] = df['text'].apply(lambda x: clean_str(str(x)))","execution_count":43,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df['text'].dropna(inplace=True)\ndf['text'] = [entry.lower() for entry in df['text']]\ndf['text']= [word_tokenize(entry) for entry in df['text']]\ntag_map = defaultdict(lambda : wn.NOUN)\ntag_map['J'] = wn.ADJ\ntag_map['V'] = wn.VERB\ntag_map['R'] = wn.ADV\nfor index,entry in enumerate(df['text']):\n    Final_words = []\n    word_Lemmatized = WordNetLemmatizer()\n    for word, tag in pos_tag(entry):\n        if word not in stopwords.words('english') and word.isalpha():\n            word_Final = word_Lemmatized.lemmatize(word,tag_map[tag[0]])\n            Final_words.append(word_Final)\n    df.loc[index,'text_final'] = str(Final_words)","execution_count":44,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_test_Tfidf = Tfidf_vect.transform(df['text_final'])","execution_count":45,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"predictions_nb = Naive.predict(X_test_Tfidf)","execution_count":46,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"predictions_nb = pd.DataFrame(predictions_nb, columns=['target'])","execution_count":47,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test['target'] = predictions_nb['target']","execution_count":48,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"results_nb = test[['id', 'target']]","execution_count":50,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"results_nb.to_csv('/kaggle/working/results_nb.csv', index=False)","execution_count":51,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"predictions_svm = SVM.predict(X_test_Tfidf)","execution_count":52,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"predictions_svm = pd.DataFrame(predictions_svm, columns=['target'])","execution_count":57,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test['target'] = predictions_svm['target']","execution_count":58,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"results_svm = test[['id', 'target']]","execution_count":59,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"results_svm.to_csv('/kaggle/working/results_svm_1.csv', index=False)","execution_count":60,"outputs":[]}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":4}