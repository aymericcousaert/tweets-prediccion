{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\nimport re\nimport nltk.corpus\nfrom nltk.corpus import stopwords\nfrom nltk.tokenize import word_tokenize\nfrom nltk.tokenize import sent_tokenize, word_tokenize\nfrom nltk.stem import WordNetLemmatizer\n\nfrom sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn import naive_bayes\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"targets = pd.read_csv('/kaggle/input/nlp-getting-started/sample_submission.csv')\ntrain = pd.read_csv('/kaggle/input/nlp-getting-started/train.csv')\ntest = pd.read_csv('/kaggle/input/nlp-getting-started/test.csv')","execution_count":71,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df = train\ndf['text'] = df['text'].str.lower()   # pasamos a lowercase\ndf = df.drop(['id', 'location'], axis=1)","execution_count":72,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.sample(8)","execution_count":74,"outputs":[{"output_type":"execute_result","execution_count":74,"data":{"text/plain":"         keyword  \\\n5959   screaming   \n290   apocalypse   \n6533    survived   \n2155      deaths   \n4560     injured   \n3899   flattened   \n5408   panicking   \n2112       death   \n\n                                                                                                                                             text  \\\n5959   harshness follows us a\\nbetter day\\nby sarah c\\nracing thoughts with screaming sirens\\npacing back and forth for... http://t.co/prontouo91   \n290     dad bought a dvd that looks like a science doc on the front but i read the back and it's actually about the impending biblical apocalypse   \n6533   @thedailyshow mahalo nui loa for making my 20s. my generation could not have survived the (w.) bush years without you. #jonvoyage #holomua   \n2155      fco minister @tobias_ellwood condemns attack at a mosque in saudi arabia that has resulted in at least 15 deaths http://t.co/c3w95h0ozz   \n4560                                                                                             @welles_7 he was injured. he is a pro bowl back.   \n3899        @kainyusanagi @grummz @pixelcanuck and flattened raynor.  raynor was a balding imperfect biker marine not a emo generic western hero.   \n5408          @beauscoven nah man he's panicking. he just found out his brothers had it off with his now wife debbie is in hospital he's stressed   \n2112  i had no issues uploading death to smoochy or awakenings clips to @youtube but for some reason bicentennial man is being a pain in the ass.   \n\n      target  \n5959       0  \n290        1  \n6533       0  \n2155       1  \n4560       0  \n3899       0  \n5408       0  \n2112       0  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>keyword</th>\n      <th>text</th>\n      <th>target</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>5959</th>\n      <td>screaming</td>\n      <td>harshness follows us a\\nbetter day\\nby sarah c\\nracing thoughts with screaming sirens\\npacing back and forth for... http://t.co/prontouo91</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>290</th>\n      <td>apocalypse</td>\n      <td>dad bought a dvd that looks like a science doc on the front but i read the back and it's actually about the impending biblical apocalypse</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>6533</th>\n      <td>survived</td>\n      <td>@thedailyshow mahalo nui loa for making my 20s. my generation could not have survived the (w.) bush years without you. #jonvoyage #holomua</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2155</th>\n      <td>deaths</td>\n      <td>fco minister @tobias_ellwood condemns attack at a mosque in saudi arabia that has resulted in at least 15 deaths http://t.co/c3w95h0ozz</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>4560</th>\n      <td>injured</td>\n      <td>@welles_7 he was injured. he is a pro bowl back.</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3899</th>\n      <td>flattened</td>\n      <td>@kainyusanagi @grummz @pixelcanuck and flattened raynor.  raynor was a balding imperfect biker marine not a emo generic western hero.</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>5408</th>\n      <td>panicking</td>\n      <td>@beauscoven nah man he's panicking. he just found out his brothers had it off with his now wife debbie is in hospital he's stressed</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2112</th>\n      <td>death</td>\n      <td>i had no issues uploading death to smoochy or awakenings clips to @youtube but for some reason bicentennial man is being a pain in the ass.</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"# generamos una columna que indica la cantidad de links a  enlaces externos\ndf['link'] = df['text'].apply(lambda x: x.count('http'))\n\n# generamos una columna que indica la cantidad de referencias a otras cuentas de twitter\ndf['contact'] = df['text'].apply(lambda x: x.count('@'))\n\n# generamos una columna que indica la cantidad de hashtags\ndf['hashtag'] = df['text'].apply(lambda x: x.count('#'))\n\n# generamos una columna que indica la cantidad de digitos\ndf['numerics'] = df['text'].apply(lambda x: len([x for x in x.split() if x.isdigit()]))\n\n# calculamos la longitud del tweet andes de limpiar\ndf['length'] = df['text'].str.len()","execution_count":75,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# calculamos la cantidad de palabras antes de limpiar\ndef count_words(text):\n    '''\n    Funcion que toma un texto y devuelve la cantidad de palabras\n    '''\n    word_counts = len(text.split(' '))\n    return word_counts\n\ndf['words'] = df['text'].apply(count_words)","execution_count":76,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# limpiamos el texto eliminando urls, cuentas, hashtags y numeros\n\ndef clean_str(string):\n    string = re.sub(r'https?\\://\\S+', '', string)\n    string = re.sub(r'http?\\://\\S+', '', string)\n    string = re.sub(r'@\\w*\\s', '', string)\n    string = re.sub(r'#\\w*\\s', '', string)\n    string = re.sub(r'\\d', '', string)\n    return string\n\ndf['text_clean'] = df['text'].apply(lambda x: clean_str(str(x)))","execution_count":77,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# eliminamos stopwords\n\nstop = stopwords.words('english')\n\ndf['text_clean'] = df['text_clean'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stop)]))","execution_count":78,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# eliminamos signos de puntuacion y caracteres especiales\ndf['text_clean'] = df['text_clean'].str.replace('[^\\w\\s]','')","execution_count":79,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df['word_tokenize'] = df['text_clean'].apply(lambda x: word_tokenize(x))","execution_count":80,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def word_lemmatizer(text):\n    lem_text = [WordNetLemmatizer().lemmatize(i) for i in text]\n    return lem_text","execution_count":81,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# unificamos palabras que poseen la misma raiz aplicando la funcion word_lematizer\ndf['word_lemmatizer'] = df['word_tokenize'].apply(lambda x: word_lemmatizer(x))","execution_count":82,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# unificamos la lista de tokens para poder analizar el texto limpio\ndf['text_clean'] = df['word_lemmatizer'].str.join(' ')","execution_count":83,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# calculamos nuevamente la longuitud, pero ahora del texto limpio                       \ndf['length-clean'] = df['text_clean'].str.len()","execution_count":84,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df['words_clean'] = df['text_clean'].apply(count_words)","execution_count":85,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pd.set_option('display.max_colwidth', 150)","execution_count":86,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df['keyword'] = df.keyword.str.replace('%20', '_')\ndf['keyword'] = df.keyword.str.lower()","execution_count":87,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# aplicamos TF-IDF seteando un maximo de 1500 palabras\ntfidf = TfidfVectorizer(max_features=1500, lowercase=True, analyzer='word', stop_words= 'english',ngram_range=(1,1))\n\n\ntrain_vect = tfidf.fit_transform(df['text_clean'])                                             \n\n# lo pasamos a dataframe\ndf_tf_idf = pd.DataFrame(data = train_vect.todense(), columns = tfidf.get_feature_names())","execution_count":88,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# como existe la columna target en el analisis tf-idf, la modifico para hacer el concat con 'target' y que no se duplique\ndf_tf_idf['targ'] = df_tf_idf['target']\ndf_tf_idf = df_tf_idf.drop(['target'], axis=1)","execution_count":89,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df['target'].value_counts()  # chequeamos si los labels estan balanceados","execution_count":90,"outputs":[{"output_type":"execute_result","execution_count":90,"data":{"text/plain":"0    4342\n1    3271\nName: target, dtype: int64"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"# eliminamos col que contienen texto y conservamos unicamente las numericas\ndf_num = df.drop([ 'keyword', 'text', 'text_clean', 'word_tokenize', 'word_lemmatizer'], axis=1)","execution_count":91,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train = pd.concat([df_num, df_tf_idf], axis=1)  # revisar porque aparecen dos columnas llamadas target","execution_count":92,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# separamos el target del resto de los features\n\ny = df_train.target    \nX = df_train.drop('target', axis=1)","execution_count":93,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# hacemos division entre train y test para cross validation\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=7)      \n\nX_train.shape, X_test.shape, y_train.shape, y_test.shape","execution_count":99,"outputs":[{"output_type":"execute_result","execution_count":99,"data":{"text/plain":"((5329, 1508), (2284, 1508), (5329,), (2284,))"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"# estandarizamos las features\nscaler = StandardScaler()\n\nX_train = scaler.fit_transform(X_train)\n\nX_test = scaler.transform(X_test)","execution_count":102,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Naive Bayes**"},{"metadata":{"trusted":true},"cell_type":"code","source":"nb = naive_bayes.GaussianNB()\nnb.fit(X_train, y_train)","execution_count":112,"outputs":[{"output_type":"execute_result","execution_count":112,"data":{"text/plain":"GaussianNB()"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"nb_validation_predictions = nb.predict(X_test)\nnb_training_predictions = nb.predict(X_train)","execution_count":113,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"nb.score(X_train, y_train), nb.score(X_test,y_test)","execution_count":115,"outputs":[{"output_type":"execute_result","execution_count":115,"data":{"text/plain":"(0.81478701444924, 0.7631348511383538)"},"metadata":{}}]},{"metadata":{},"cell_type":"markdown","source":"**SVM**"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn import svm","execution_count":116,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"svm_ = svm.SVC(C=1.0, kernel='linear', degree=3, gamma='auto')\nsvm_.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"svm_.score(X_train, y_train), svm_.score(X_test,y_test)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":4}